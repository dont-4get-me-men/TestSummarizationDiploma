
# Script

## Вибіркові методи
#### Cтатистичні методи 
Найперші і найпростіші методи, які використовуються для реферування тексту - це статичні методи. Суть полягає в тому, щоб використовувати властивості тексту без того, щоб дивитись на взаємозвязки між словами чи реченнями. 
 
На що можна звертати увагу в тексті:
1. Використання найпопулярніших слів. Слова, яке найчастіше зустрічаються в тексі репрезентують основний концепт. Ці слова використовуються для оцінки речень, де пізніше ми вибираємо речення з найкращим балом.
2. Позиція в параграфі. За дослідженням Baxendale [4] ключову інформація в тексті надається або на початку або в кінці параграфу. Відповідно, що якщо брати перші речення з параграфу, то ти отримаєш якомога більше важливої інформації
3. Метод Едмундсона полягає в обєднанні інших статистичних методів. Едмундсон розглядав комбінацію декількох методів:
	- Популярність ключових слів (Key) -  описано в пункті 1 
	- Позиція речення в тексті (Location) - описано в пункті 2
	- Метод заголовка (Title) - де слова з заголовка тексту чи параграфа проганяються через Null dictionary (список слів, які не несуть ніякої емоційної забарвленості для тексту). Після чого ми формуємо список слів, що не потрапили в Null dictionary.
	- Метод ключових фраз (Cue) - беремо слова, які є в тексті і в залежності від того чи вони потрапляють в категорію (Bonus, Null, Stigma), де в залежності від категорії їм розраховують ваги. Далі для кожного речення обраховують вагу кожного речення і рангують.
	За методом Едмундсона нова оцінка речення буде складати з себе лінійну комбінацію рахунків з попередніх методів. Коефіцієнти при рахунках - це гіперпараметри, які обирає дослідник.
	Результати такого алгоритму показали, що найкраще показують себе Cue-Title-Location алгоритми (Див. Додаток Малюнок 1)


#### Методи класичного машинного навчання
Ідея цих методів полягає в тому, щоб використовувати задача класифікації для прогнозування "важливого" речення. Найбільша проблема в задачах такого типу (як в цілому подальших алгоритмів) це правильна репрезентація речення як вектора з чисел ( ембедінг). 
Серед рзіних варіацій ембедінгів є:
1. TF-IDF. Метод що базується на двох параметрах: частоти терміну (term frequency) і обернена частоту документа (inverse document frequency).
	 TF - репрезентує частота терміну серед всіх  термінів
	 IDF - репрезентує коефіцієнт рідкості слова серед документів. 
	 $\text{TF}(t,d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d}$
	 $\text{IDF}(t,D) = \log\left(\frac{\text{Total number of documents in the corpus } N}{\text{Number of documents containing term } t}\right)$

2. BOW(Bag Of Words)  - мішочок слів. 
	 Нехай у нас є словник, який складається з всіх слів, які є в всіх документах.
	 Кожне речення буде репрезентуватись які вектор, де у навпроти колонки з кожним словом буде число входжень цього слова в реченні [12]
3. CBOW - продовжений мішочок слів:
   Використовується для прогнозування слова, яка знаходиться в центрі. Кожне слово на вході - це one-hot вектор. ( вектор з однією одиницею і всіма іншими нулями). Надалі ми беремо лінійну комбінацію цих векторів , застосовуємо для результату алгорим softmax , який на виході буде давати ймовірність прогнозованого слова. З таблиці цієї ймовірності ми і будемо вибирати наше ключове слово [15] 
1. 

##### Наївний баєсів класифікатор
Як уже було сказано раніше цей метод будується на задачі класифікації, де в нас є датасет для тренування, що складається з підходящих та не підходящих для реферування речень. З тексту також витягуються основні ознаки: частоти слів, кількість слів з великої букви, довжина рядка, позиція в параграфі, тощо [6]. 
Далі використовуючи формулу баєса ми для кожного речення визначаємо чи ймовірність потрапити в реферування використовуючи формулу нижче. 

$P(s \in S|F_1,F_2,...,F_k) = \frac{P(s \in S) \times \prod_{j=1}^{k} P(F_j| s \in S)}{\prod_{j=1}^k{P(F_j)}}$
 $P(s \in S)$ - ймовірність речення потрапити в реферування ( завжди константа)
 $P( F_j )$ - ймовірність ознаки  $F_j$
 $P(F_j|s \in S)$ - ймовірність ознаки $F_j$ за умови, що речення потрапило до реферування

Використання даного методу значно краще показує себе відносно інших методів класифікації зокрема в [7], де результат був показаний кращий за алгоритм дерева рішень C4.5, а також. 
В середньому такі алгоритми набирають 40-50 відсотків точності [7],[8] (див додаток малюнки 2,3)

Для прогнозування ключового речення можна також використовувати інші алгоритми для класифікацїі, зокрема SVM 

##### Ансамблі класичних методів 

##### Штучна нейронна мережа (ANN)
Не можна було б згадати використання нейронних мереж, зокрема найпопулярнійший варіант нейронної мережі [9]. 

Вхідними даними в даному випадку будуть речення в документі, де в кожному реченні буде наступний набір фіч:
1. Абзац після заголовка
2. Розташування абзацу в документі 
3. Розташування речення в абзаці 
4. Перше речення в абзаці 
5. Довжина речення 
6. Кількість тематичних слів у реченні 
7. Кількість слів заголовка в реченні

В статті для тренування використовували метод спряженого градієнта зі структурою нейронної мережі з шарами 7-6-1. В процесі тренування моделі через використання штрафної функції деякі нейрони були вилучені після чого деякі нейрони об'єднувались в кластери, де тепер активаційне значення цих нейронів отримуються з центроїда.  

В досліджуваному папері результати були доволі високими ( 96 відсотків) з приблизно 1500 досліджуваних речень ( 25 відсотків ключових речень).    
#### Семантичні методи 
Не завжди фічі, які ми використовуємо для реферування тексту гарно репрезентують текст, адже не обовязково, щоб речення з найпопулярнішими словами несли багато змісту.

Через це семантичні методи намагаються інедтифікувати взаємовідносини слів між собою за допомогою граматичного аналізу, позначення частини мов, тощо. 
##### Метод лексичних ланцюгів

Доволі часто в тексті концепти, які були вказані раніше пізніше згадуються з використанням інших слів. Таким чином було запропоновано ці слова, які відсилаються один на одного комбінувати в ланцюги. 

Як модерувати ланцюг?
1. З самого початку ланцюг у нас пустий. 
2. Додаємо перше слово. 
3. Беручи нове слово ми перевіряємо чи воно зєднано з іншими словами в ланцюгах. Слова зєднані, якщо вони відповідають одному з декількох критерїв описаних в [10]. В основному це якщо слова знаходяться в одній чи зєднаних категоріях тезаурусу (в [11] використовувався WordNet).
4. У випадку, якщо ніякого звязку немає, то ми створюємо нову категорію 

Наступне, що нам треба зробити - це вибрати найкращі ланцюги зі всіх, які є. Зокрема в [11] вони використовуються оцінку виведену після ручного вибору найвагоміших ланцюгів. 
Емпірично було визначено, що найкращий прогноз вагомих ланцюгів складається з "Довжини" - кількість входжень членів ланцюга та "індекс однорідності" 1 - кількість унікальних випадків поділена на довжину.
Оцінка визначається за формулою 
$Score(chain) = Lenght*Homogeneity$

Ми вибираємо ланцюги, які при цьому мають оцінку, яка лежить поза середнім + 2 стандартних відхилення. 

Речення можна вибирати декілька способами зокрема в [11] перевіряють 3 гіпотези серед яких найкраще показує те, де ми беремо перше речення для кожного слова з ланцюга. 
##### TextRank

Один із перших методів навчання без учителя в сфері виділення ключових слів, а також реферування тексту, який був вперше представлений в далекому 2004 році, який створений на базі алгоритму PageRank [2]

Ідея алгоритму полягає в створенні графу залежностей між деякими структурами ( будь-то веб-сторінки, слова чи цілі речення). В нашому випадку відбувається наступне:
1. Кожне речення в тексті беремо як вершину графу
2. Кожне зєднання між нашими вершинами - це коефіцієнт, який репрезентує схожість двох речень. ( в оригінальній статті  це формула нижче ) ми також можемо використовувати і інші формули (косинусна схожість, cтрічкове ядро, найдовша підпослідовність)
$Similarity(S_i,S_j) =\frac{\omega_k|\omega_k \in S_i \cap S_j}{log(|S_i|)+log(|S_j|)}$
3. Після перших двох пунктів у нас формується граф з  вагами, на якому ми використовуємо PageRank алгоритм. 
	$Score(v_i) = (1 - d) + d \times \sum_{v_j \in In(v_i)} \frac{w_{ji}}{\sum_{v_k \in Out(v_j)} w_{jk}} \times Score(v_j)$
- Score(vi​) - оцінка вершини vi​.
- d - коефіцієнт згасання (зазвичай встановлюється на значення 0.85).
- In(vi​) - множина вершин, які вказують на вершину vi​.
- Out(vj​) - множина вершин, на які вказує вершина vj​.
- wji​ - вага ребра від вершини vj​ до vi​.
. 
4. Після декількох ітерацій алгоритму ми сортуємо за оцінкою наші реченні і кращі перші використовуємо для створення короткого опису



##### Lex Rank
В базі цього алгоритму у нас лежить матриця схожості всіх речень, де схожість визначається за формулою:
$\text{IDF-Modified Cosine Similarity}(S_1, S_2) = \frac{{\sum_{i=1}^{n} (tf_{1,i} \times idf_i) \times (tf_{2,i} \times idf_i)}}{{\sqrt{\sum_{i=1}^{n} (tf_{1,i} \times idf_i)^2} \times \sqrt{\sum_{i=1}^{n} (tf_{2,i} \times idf_i)^2}}}$
, де:
- S1​ і S2 - це два порівнювальні речення
- n - це кількість унікальних термінів в словнику.
-  tf_1, tf2 - це частоти термінів в реченнях S1​ і S2
- idf -  коефіцієнт рідкості слова

Аналогічно попередньому алгоритму цей також працює на основі графу. Як і в TextRank у нас кожне речення репрезентує одну вершину графу, якій в подальшому ми даємо певну оцінку. Чим більша оцінка тим більш вагомим є речення.  
В оригінальній статті було  запропоновано і порівняно  три варіанти.
1. Ступінь центральності (Centrality degree)
   Після побудови матриці схожості ми прибираємо ті звязки, які вище зазначеного мінімуму(трешхолду). Після такої фільтрації ми шукаємо яка кількість речень має звязки вище цього трешхолду і рахуємо кількість таких звязків. Ті речення, які мають найбільше звязків ми відбираємо для сумаризації.  
1. LexRank
	 Головна проблема попереднього методу в тому, що деякі речення, які не стосуються головної теми можуть підняти рейтинг один одного. Тому була запропонована ідея, де речення буде вибиратись не лише за рахунок кількості його друзів, але й завдяки їхньої вагомості. 
	 Найпростіше , що можна зробити - це дати кожному речення оцінку, яку він буде віддавати іншим сусідам. 
	 $p(u) = \sum_{v \in adj[u]}\frac{p(v)}{deg(v)}$
	 p(i) - оцінка речення i
	 adj[u] - сусіди речення u
	 deg(i) - ступінь речення i

	 Алгоритм покращується, якщо застосувати формулу з PageRank [2]. Після застосування алгоритм. Псевдокод можна глянути в нижче 
	 ![[algoLexRank.png]]
	 Малюнок 4.  Псевдокод алгоритму LexRank
	 
1. Continuous LexRank
	 Проблема звичайного LexRank в тому, що граф на базі якого у нас будується алгоритм - незважений. Можна покращити алгоритм, якщо враховувати нормалізовані ваги ребер між графами 
		$p(u)=\frac{d}{N}+(1-d)\sum_{v \in adj[u]}\frac{idf-modified-cosine(u,v)}{\sum_{z \in adj[v]}{idf-modified-cosine(z,v)}}p(v)$






#### Методи з використанням нейронних мереж
На відміну від класичних методів, алгоритми з використанням нейронних мереж з меншою кількістю людської залученості. 

Етапи тренування моделі:
1. Як і в класичних методів перший етап - це репрезентування слів як векторів(Word2Vec, CW vectors, and GloVe).
2. Речення представляються у вигляді продовжених( continuous) векторів з використанням словесних ембедінгів
3. Ембедінги з минулих етапів направляємо до подальшої моделі

Послідуючи методи будуть використовувати або CNN або RNN або їх комбінацію.

##### Continuous Vector Space Models
Головна ідея метода полягає в тому, що нам варто максимізувати значення показника, що репрезентує різноманітність та покриття тексту:
$F(S) = L(S) + \lambda R(S)$
, де  L(S) - покриття реферування, а R(S) - функція різноманітності

  в [15] словесних ембедінги були в  таких варіантах:
1. СW вектори - стоворені за допомогою максимізації функції втрат $max(0, 1 − s(x) + s( \hat{x}))$, де x - дійсна  n-грамма  і пошкоджена n-грамма.
2. Продовжені пропуск-грамми (skip-grams) - модель створена для прогнозування контексту навколо слова

Ембедінги фраз:
	1. Сума  векторів $x_p = \sum_{x_w \in sentence}x_w$  
3. RAE - рекурентний автоенкодер - спочатку обєднує слова в реченні додаючи дві різних синтаксичних конструкцій ( енкодує), а потім декодує його. 
   ![[RAE.png]]
    Малюнок 5. структура RAE.
В папері [15]  було розглянуто різні поєднання  алгоритмів, які ми розглянемо в практичній частині з результами. 
##### CNNLM

В цьому алгоритмі у нас є 2 вагомих момента:
1. Тренування словесних ембедінгів:
	Ми використовуємо архітектуру CNN де на вході у нас використовуються передтреновані cловесні ембедінги, де пізніше ми тренуємо за допомогою навчання без учителя з використанням noise-contrastive estimation (NCE), де модель вчиться прогнозувати наступні слова без захламляючих слів.
	![[cnn.png]]
	Малюнок 6. Структура CNNLM
1. Вибір речення визначається 
	 $arg max_{|C|=k} Q(C) = \alpha \sum_{i \in C}p^2_i -  \underset{i,j \in C}{\sum}p_iS_{i,j}p_j$
	 , де альфа is додатній регуляризацій параметр that defines the trade-off between the two terms, 
	 Si,j - матриця схожості
	 i.e., prestige and diversity, and 
	 pi -   і-те входження престижного вектора p після застосування алгоритму PageRank

Результатом алгоритму буде прорангований список з речень з яких ми і вибиремо найкращі. 

##### SUMMARuNNer

Структура моделі наступна:
Наша модель складається з двошарового двонаправленого GRU-RNN, що можна переглянути на малюнку 7. 

Перший рівень - працює на рівні слів обчислюючи приховані представлення в позиції слова. 
Другий рівень працює на рівні речень, який приймає на рівні входу обєднані дані з минулого рівня. Ці дані кодують представлення речення в документі. Надалі весь документ представляється як нелінійне перетворення обєднання прихованих станів. 
$d = \tanh\left(\mathbf{W}_d \cdot \frac{1}{N_d} \sum_{j=1}^{N_d} [h_{fj}, h_{bj}] + \mathbf{b}\right)$
, де 
- Wd​ - матриця ваг.
- Nd​ - кількість речень у документі.
- h_fj​ і hbj​ - приховані стани, що відповідають j-му реченню відповідно до прямого та зворотного рівнів RNN для речень.
- [][] - означає конкатенацію векторів.
- bb - вектор зсуву.

![[summarunner neural.png]]
Малюнок 7. Стурктура SummaRuNNer

Вибір речення для стислого змісту відбувається за допомогою класифікації з використанням формули, яка показана нижче:
$P(y_j = 1 | h_j, s_j, d) = \sigma\left( \mathbf{W}_{c} h_j + h_j^T \mathbf{W}_{s} s_j - h_j^T \mathbf{W}_{r} \tanh(s_j) + \mathbf{W}_{\text{appa}} j + \mathbf{W}_{\text{rppr}} j + \mathbf{b} \right)$
, де 
hj - поточне зважене підсумування прихованих станів речень до j-го речення. 
W_ch_j представляє інформаційний зміст j-го речення, 
h^T_j Wsd позначає помітність речення по відношенню до документа,
hT j Wr tanh(sj) фіксує надмірність речення по відношенню до поточного стану резюме.

Оскільки датасети рідко містять вибіркове реферування, тому в  [16] пішли шляхом навчання без учителя на датасетах з абстрактними резюме. Підхід бахується на виборі речень з документа з максимізацією rouge метрики. Зупинка додавання нових речень відбувається коли додавання нового кандидата не покращує цільову метрику. 

### Абстарктні методи

Абстракті методи - це методи, які полягають в тому, щоб витягнути головну суть з тексту в тому числі перефразовуючи його. Для того, щоб це зробити з гарною якістю нам варто звернути увагу на :
	1. Яким чином представити 

Абстрактивні підсумовувачі зосереджені на захопленні представлення значення всього документа, а потім генерують абстрактне резюме на основі цього представлення значення. Таким чином, нейронно-базовані абстрактні підсумовувачі, які є методами, заснованими на генерації, повинні прийняти наступні два рішення: 1) як представити весь документ за допомогою кодера; 2) як згенерувати послідовність слів декодером. У цьому розділі ми розглядаємо п’ять абстрактних нейронних суматорів у хронологічному порядку. Кожна система підсумовування представлена на основі свого кодера та свого декодера. Наприкінці цього розділу узагальнено методи, які використовуються в абстрактних нейронних моделях, і порівняно ефективність моделей.
### Додаток

![[image-11-x71-y83.png]]
Малюнок 1. Середні оцінки 
![[image-6-x309-y610.png]]
Малюнок 2. Результати [8]
![[image-7-x121-y304.png]]
Малюнок 3. Результати з [7]



### Refenences
[1] TextRank: Bringing Order into Text [[mihalceaTextRankBringingOrder2004]]
[2] The anatomy of a large-scale hypertextual Web search engine [[brinAnatomyofLargescalehypertextualWebSearchengine]]
[3] A survey on Automatic Text Summarization [[nazariSurveyAutomaticText2019]]
[4] Baxendale, P. B. (1958). Machine- Made Index for Technical Literature. An Experiment. IBM Journal of Research Development, vol. 4, no. 2, pp. 354-361. 
[5] Edmondson, H. P. (1969). New Methods in Automatic Extracting. Journal of the ACM, vol. 2, no. 16, pp. 264-285. [[edmundsonNewMethodsAutomatic1969]]
[6] A Trainable Document Summarizer, Julian Kupiec, Jan Pedersen and Francine Chen, 1995
[7] Automatic Text Summarization Using a Machine Learning Approach
Author: Joel Larocca Neto, Alex A. Freitas, Celso A. A. Kaestner [[netoAutomaticTextSummarization2002]]
[8] A Trainable Document Summarizer Using Bayesian Classifier Approach, Aditi Sharan , Hazra Imran, ManjuLata Joshi, 2008 [[sharanTrainableDocumentSummarizer2008]]
[9] Text Summarization Using Neural Networks, KHOSROW KAIKHAH [[kaikhahTextSummarizationUsing]]
[10]Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms, Graeme Hirst, David St-onge, 1995
[11] Using Lexical Chains for Text Summarization, Regina Barzilay, Michael Elhadad, 1997
[12 ] Gentle inroduction to bag of Words https://machinelearningmastery.com/gentle-introduction-bag-words-model/
[13] LexRank: Graph-based Lexical Centrality as Salience in Text Summarization, G. Erkan, D. R. Radev, 2004
[14] Understanding the Continuous Bag https://medium.com/@codethulo/understanding-the-continuous-bag-of-words-cbow-model-architecture-working-mechanism-and-math-78c7284a8d5a#:~:text=Overall%2C%20the%20Continuous%20Bag%20of,text%20classification%20and%20language%20translation.
[15]Extractive Summarization using Continuous Vector Space Models, Mikael Kågebäck, Olof Mogren, Nina Tahmasebi, Devdatt Dubhashi, 2014 [[kagebackExtractiveSummarizationUsing2014]]
[16] Optimizing Sentence Modeling and Selection for Document Summarization, Wenpeng Yin , Yulong Pei [[yinOptimizingSentenceModeling]]
[17] SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents, Ramesh Nallapati, Feifei Zhai, Bowen Zhou, 2016 [[nallapatiSummaRuNNerRecurrentNeural2016]]
[18]